# -*- coding: utf-8 -*-
"""[Comp Bio] Benchmarking Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xve2nIAufOqE2a4QF6QjheejNNxMYzXu
"""

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV
import numpy as np
import pandas as pd

"""## Dataset 1"""

selected_features_dataset = pd.read_csv("selected_features_dataset_1.csv")

y = selected_features_dataset.iloc[:, -1].values
X = selected_features_dataset.iloc[:, :-1].values

labels = np.unique(y)
anomaly_label = labels[0]

for label in labels:
    print(f"Class {label}: {np.sum(y == label)} samples")
    if (np.sum(y == label) < X.shape[0] - np.sum(y == label)):
      anomaly_label = label

"""## Dataset 2"""

selected_features_dataset = pd.read_csv("selected_features_dataset_2.csv")

y = selected_features_dataset.iloc[:, -1].values
X = selected_features_dataset.iloc[:, :-1].values

labels = np.unique(y)
anomaly_label = labels[0]

for label in labels:
    print(f"Class {label}: {np.sum(y == label)} samples")
    if (np.sum(y == label) < X.shape[0] - np.sum(y == label)):
      anomaly_label = label

selected_features_dataset = pd.read_csv("augmented_dataset_rnsa_2.csv")

y = selected_features_dataset.iloc[:, -1].values
X = selected_features_dataset.iloc[:, :-1].values

labels = np.unique(y)
anomaly_label = labels[0]

for label in labels:
    print(f"Class {label}: {np.sum(y == label)} samples")
    if (np.sum(y == label) < X.shape[0] - np.sum(y == label)):
      anomaly_label = label

selected_features_dataset = pd.read_csv("augmented_dataset_vdnsa_2.csv")

y = selected_features_dataset.iloc[:, -1].values
X = selected_features_dataset.iloc[:, :-1].values

labels = np.unique(y)
anomaly_label = labels[0]

for label in labels:
    print(f"Class {label}: {np.sum(y == label)} samples")
    if (np.sum(y == label) < X.shape[0] - np.sum(y == label)):
      anomaly_label = label

selected_features_dataset = pd.read_csv("augmented_dataset_hdnsa_2.csv")

y = selected_features_dataset.iloc[:, -1].values
X = selected_features_dataset.iloc[:, :-1].values

labels = np.unique(y)
anomaly_label = labels[0]

for label in labels:
    print(f"Class {label}: {np.sum(y == label)} samples")
    if (np.sum(y == label) < X.shape[0] - np.sum(y == label)):
      anomaly_label = label

"""## Random Data Augmentation"""

selected_features_dataset.shape

def augment_random_points(df, n_points, anomaly_label):
    df_dropped = df.copy()
    df_dropped = df_dropped.drop(columns=['diagnosis'])
    np.random.seed(42)

    min_vals = df_dropped.min()
    max_vals = df_dropped.max()

    print(min_vals)
    print(max_vals)

    n_features = df_dropped.shape[1]
    print(n_features)

    random_points = np.random.uniform(low=min_vals.values, high=max_vals.values, size=(n_points, n_features))

    augmented_df = pd.DataFrame(random_points, columns=df.columns[:-1])
    augmented_df['diagnosis'] = anomaly_label
    augmented_df = pd.concat([df, augmented_df], ignore_index=True)

    return augmented_df

random_augmented_data = augment_random_points(selected_features_dataset, 150, anomaly_label)

y = random_augmented_data.iloc[:, -1].values
X = random_augmented_data.iloc[:, :-1].values

"""## Train test split"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

"""## Logistic Regression Classifier"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = LogisticRegression(max_iter=200, solver='lbfgs', random_state=42)
model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)

acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""## Gaussian Naive Bayes"""

model = GaussianNB()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""## Random Forest Classifier"""

model = RandomForestClassifier(
    n_estimators=200,  # number of decision trees
    max_depth=8,       # control model complexity
    random_state=42
)

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 8, None],
    'min_samples_split': [2, 5]
}

best_model = model

best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
y_proba = best_model.predict_proba(X_test)[:, 1]

print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""## Multi Layer Perceptron"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

mlp = MLPClassifier(hidden_layer_sizes=(50, 20),
                    activation='relu',
                    solver='adam',
                    alpha=1e-4,
                    max_iter=300,
                    random_state=42)
mlp.fit(X_train_scaled, y_train)

y_pred = mlp.predict(X_test_scaled)

print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("Classification Report:")
print(classification_report(y_test, y_pred))

y_proba = mlp.predict_proba(X_test_scaled)
print("Predicted probabilities for the first 5 samples:")
print(y_proba[:5])