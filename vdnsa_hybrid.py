# -*- coding: utf-8 -*-
"""vdnsa hybrid

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nW6CMKIt3xV2qpHVofpFE1wTloV2joYp
"""

# Cell 1: Setup and Function Definitions
!pip install numpy pandas matplotlib # Ensure libraries are installed

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches # For detector visualization
import random
import time
import warnings # To handle cases where fewer anomalies are generated than requested
from google.colab import drive # If using Google Drive

# --- Configuration ---
N_FEATURES = 2 # <<<<<<< CHANGED TO 2 >>>>>>>>>

# --- Helper Function: Check if a point matches a variable detector ---
def matches_detector_vdnsa(point, detector_center, detector_radii):
    """Checks if a point falls within the hyper-rectangular region of a detector."""
    # Check if the absolute difference along each dimension is within the corresponding radius
    # Works correctly for N_FEATURES = 2
    return np.all(np.abs(point - detector_center) <= detector_radii)

# --- Phase 1: Detector Generation ---
def generate_detectors_vdnsa(self_data, num_detectors, min_radius, max_radius, max_trials=10000):
    """Generates variable detectors that do not match any self data points."""
    if self_data.ndim != 2 or self_data.shape[1] != N_FEATURES:
        raise ValueError(f"self_data must be a 2D array with {N_FEATURES} features (columns). Shape received: {self_data.shape}")

    mature_detectors = []
    n_self_samples = self_data.shape[0]
    data_min = np.min(self_data, axis=0)
    data_max = np.max(self_data, axis=0)

    # Ensure min/max radius are arrays of size N_FEATURES
    if isinstance(min_radius, (int, float)):
        min_radius = np.full(N_FEATURES, min_radius) # Will be size 2
    elif len(min_radius) != N_FEATURES:
         raise ValueError(f"min_radius must be a float or a list/tuple of size {N_FEATURES}")
    else:
         min_radius = np.array(min_radius)

    if isinstance(max_radius, (int, float)):
        max_radius = np.full(N_FEATURES, max_radius) # Will be size 2
    elif len(max_radius) != N_FEATURES:
         raise ValueError(f"max_radius must be a float or a list/tuple of size {N_FEATURES}")
    else:
         max_radius = np.array(max_radius)

    if np.any(min_radius > max_radius):
        raise ValueError("min_radius cannot be greater than max_radius for any dimension.")
    if np.any(min_radius < 0):
        raise ValueError("min_radius cannot be negative.")


    print(f"Starting detector generation for {num_detectors} detectors ({N_FEATURES} features)...")
    start_time = time.time()
    trials = 0

    while len(mature_detectors) < num_detectors and trials < max_trials:
        trials += 1

        # 1. Generate candidate detector center within data bounds (or slightly expanded)
        range_expansion = (data_max - data_min) * 0.1 # Expand by 10%
        gen_min = data_min - range_expansion
        gen_max = data_max + range_expansion
        # Generate a 2-element center vector
        candidate_center = np.random.uniform(gen_min, gen_max, size=N_FEATURES)

        # 2. Generate candidate detector radii within specified range
        # Generate a 2-element radii vector
        candidate_radii = np.random.uniform(min_radius, max_radius, size=N_FEATURES)

        # 3. Check if the candidate detector matches any self point
        is_valid = True
        for i in range(n_self_samples):
            self_point = self_data[i, :]
            if matches_detector_vdnsa(self_point, candidate_center, candidate_radii):
                is_valid = False
                break # No need to check other self points

        # 4. If it doesn't match any self point, add it to mature detectors
        if is_valid:
            mature_detectors.append({
                'center': candidate_center, # Will be size 2
                'radii': candidate_radii   # Will be size 2
            })
            if len(mature_detectors) % (num_detectors // 10 + 1) == 0: # Print progress
                 print(f"  Generated {len(mature_detectors)} / {num_detectors} detectors... (trial {trials})")


    end_time = time.time()
    print(f"Detector generation finished in {end_time - start_time:.2f} seconds.")
    print(f"Generated {len(mature_detectors)} detectors after {trials} trials.")

    if trials >= max_trials and len(mature_detectors) < num_detectors:
        warnings.warn(f"Warning: Reached max_trials ({max_trials}) but only generated "
                      f"{len(mature_detectors)} out of {num_detectors} desired detectors. "
                      "Consider increasing max_trials, adjusting radius ranges, or checking self-data distribution.")

    return mature_detectors

# --- Phase 2: Monitoring / Classification ---
# (No changes needed, relies on N_FEATURES indirectly via detector/point shapes)
def classify_point_vdnsa(point, mature_detectors):
    """Classifies a data point as normal (self) or anomaly (non-self)."""
    if point.shape[0] != N_FEATURES:
         raise ValueError(f"Point must have {N_FEATURES} features.")

    for detector in mature_detectors:
        if matches_detector_vdnsa(point, detector['center'], detector['radii']):
            return 'anomaly' # Matched a detector -> anomaly

    return 'normal' # Matched no detectors -> normal


# --- NEW PHASE: Synthetic Anomaly Generation ---
# (No changes needed, relies on N_FEATURES indirectly via detector shapes)
def generate_synthetic_anomalies(mature_detectors, num_synthetic_anomalies):
    """Generates synthetic anomaly data points based on the mature detectors."""
    if not mature_detectors or num_synthetic_anomalies <= 0:
        return np.empty((0, N_FEATURES)) # Will return shape (0, 2)

    num_available_detectors = len(mature_detectors)
    num_to_generate = min(num_synthetic_anomalies, num_available_detectors)

    if num_synthetic_anomalies > num_available_detectors:
        warnings.warn(f"Requested {num_synthetic_anomalies} synthetic anomalies, but only "
                      f"{num_available_detectors} detectors are available. Generating {num_to_generate} anomalies.")

    synthetic_points = []
    chosen_indices = np.random.choice(num_available_detectors, size=num_to_generate, replace=True)

    print(f"\nGenerating {num_to_generate} synthetic anomalies...")
    for i in chosen_indices:
        detector = mature_detectors[i]
        center = detector['center'] # size 2
        radii = detector['radii']   # size 2

        min_bounds = center - radii
        max_bounds = center + radii

        # Generate a random point uniformly within the 2D rectangle
        synthetic_point = np.random.uniform(min_bounds, max_bounds, size=N_FEATURES)
        synthetic_points.append(synthetic_point)

    print("Synthetic anomaly generation complete.")
    return np.array(synthetic_points) # Will return shape (num_generated, 2)

# Cell 2: Mount Drive (Optional) and Load Data

# --- Option A: Mount Google Drive (Recommended for persistence) ---
# drive.mount('/content/drive')
# data_base_path = '/content/drive/MyDrive/Colab_Data/' # <<< ADJUST PATH in your Drive

# --- Option B: Use Colab Temporary Storage (Upload file manually) ---
data_base_path = '/content/' # Base path for direct uploads

# --- Specify your DATASET filename (must have 2 features) ---
data_filename = data_base_path + 'selected_features_dataset_5.csv' # <<< CHANGE FILENAME

print(f"Attempting to load self data from: {data_filename}")
try:
    # Using Pandas (Recommended - handles headers automatically)
    df = pd.read_csv(data_filename, delimiter=',') # Adjust delimiter if needed
    df = df.drop(df.columns[-1], axis=1)
    df = (df - df.min()) / (df.max() - df.min())

    # Check number of columns BEFORE converting to NumPy
    if df.shape[1] != N_FEATURES:
         raise ValueError(f"Loaded DataFrame has {df.shape[1]} columns, but expected {N_FEATURES}.")

    self_data = df.values # Convert the Pandas DataFrame to a NumPy array AFTER check

    print(f"Successfully loaded {self_data.shape[0]} samples with {self_data.shape[1]} features.")

except FileNotFoundError:
    print(f"Error: Data file '{data_filename}' not found.")
    print("Check the path: Did you upload the file? Did you mount Drive correctly? Is the filename correct?")
    raise # Stop execution
except ValueError as ve: # Catch the specific column count error
    print(f"Data Loading Error: {ve}")
    raise # Stop execution
except Exception as e:
    print(f"Error loading data file: {e}")
    print("Check file format, delimiter, and ensure it contains only numeric data.")
    raise # Stop execution

# Display basic info about the loaded data
print(f"\nOriginal Self Data Shape: {self_data.shape}")
print(f"Self Data Min per feature: {np.min(self_data, axis=0)}")
print(f"Self Data Max per feature: {np.max(self_data, axis=0)}")
print("-" * 30)

# Cell 3: Generate Detectors and Synthetic Anomalies

# --- Hyperparameters ---
NUM_TARGET_DETECTORS = 466  # Adjust as needed
# Radii need to be length 2 now or single floats
MIN_RADIUS_PER_DIM = [0.1, 0.1] # <<< EXAMPLE: List of size 2
MAX_RADIUS_PER_DIM = [0.8, 1.2] # <<< EXAMPLE: List of size 2
# Or use single floats:
# MIN_RADIUS_PER_DIM = 0.1
# MAX_RADIUS_PER_DIM = 1.0
MAX_GENERATION_TRIALS = 40000 # Adjust as needed
NUM_SYNTHETIC_ANOMALIES = 466 # Adjust as needed
# --- End Hyperparameters ---

# 1. Generate Detectors
detectors = generate_detectors_vdnsa(
    self_data,
    NUM_TARGET_DETECTORS,
    MIN_RADIUS_PER_DIM,
    MAX_RADIUS_PER_DIM,
    MAX_GENERATION_TRIALS
)
print("-" * 30)

# 2. Generate Synthetic Anomalies using Detectors
if not detectors:
    print("No detectors were generated. Cannot generate synthetic data.")
    synthetic_anomalies = np.empty((0, N_FEATURES)) # Create empty array
else:
    synthetic_anomalies = generate_synthetic_anomalies(detectors, NUM_SYNTHETIC_ANOMALIES)

if synthetic_anomalies.shape[0] > 0:
    print(f"Generated Synthetic Anomaly Data Shape: {synthetic_anomalies.shape}")
else:
    print("\nNo synthetic anomalies were generated (either 0 requested or no detectors available).")

# 3. Combine Original Self Data and Synthetic Anomalies
combined_data = np.vstack((self_data, synthetic_anomalies))

# Create corresponding labels (0 for self, 1 for synthetic anomaly)
self_labels = np.zeros(self_data.shape[0], dtype=int)
anomaly_labels = np.ones(synthetic_anomalies.shape[0], dtype=int)
combined_labels = np.concatenate((self_labels, anomaly_labels))

print(f"\nCombined Dataset Shape: {combined_data.shape}")
print(f"Combined Labels Shape: {combined_labels.shape}")
print(f"Number of original self samples in combined set: {np.sum(combined_labels == 0)}")
print(f"Number of synthetic anomalies in combined set: {np.sum(combined_labels == 1)}")
print("-" * 30)

# Cell 4: 2D Visualizations

# Extract feature names (if available from DataFrame 'df', otherwise use generic names)
try:
    # Get column names from the DataFrame used for loading
    # Directly use indices 0 and 1 for 2 features
    feature_name1 = df.columns[0]
    feature_name2 = df.columns[1]
except NameError:
    feature_name1 = 'Feature 1'
    feature_name2 = 'Feature 2'
except IndexError: # Fallback if df had fewer than 2 columns (should have been caught earlier)
    feature_name1 = 'Feature 1'
    feature_name2 = 'Feature 2'

print(f"\n--- Generating 2D Visualizations using '{feature_name1}' (X-axis) and '{feature_name2}' (Y-axis) ---")

# --- Plot 1: Original Self Data ---
print("Generating Plot 1: Original Self Data...")
plt.figure(figsize=(8, 6))
plt.scatter(self_data[:, 0], self_data[:, 1], # Use indices 0 and 1
            label='Original Self Data', alpha=0.7, s=30, c='blue')
plt.xlabel(feature_name1)
plt.ylabel(feature_name2)
plt.title('Plot 1: Original Self Data Points (2 Features)')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.axis('equal') # Often useful for 2D anomaly detection plots
plt.show()


# --- Plot 2: Original Data + Detectors (Centers and Radii) ---
print("\nGenerating Plot 2: Original Self Data + Detectors...")
fig, ax = plt.subplots(figsize=(10, 8))

# Plot self data first
ax.scatter(self_data[:, 0], self_data[:, 1], # Use indices 0 and 1
           label='Original Self Data', alpha=0.6, s=30, c='blue', zorder=1)

# Determine plot limits based on data and detector extents
min_x, max_x = np.min(self_data[:, 0]), np.max(self_data[:, 0])
min_y, max_y = np.min(self_data[:, 1]), np.max(self_data[:, 1])

detector_centers_x = []
detector_centers_y = []

if detectors: # Check if detectors list is not empty
    for detector in detectors:
        center = detector['center'] # size 2
        radii = detector['radii']   # size 2

        # Get center and radii for the selected dimensions (0 and 1)
        center_x, center_y = center[0], center[1]
        radius_x, radius_y = radii[0], radii[1]

        detector_centers_x.append(center_x)
        detector_centers_y.append(center_y)

        # Create an Ellipse patch (representing the rectangular detector area in 2D)
        ellipse = patches.Ellipse((center_x, center_y), width=2 * radius_x, height=2 * radius_y,
                                  edgecolor='red', facecolor='none', linestyle='-', linewidth=1,
                                  alpha=0.5, label='_nolegend_', zorder=2)
        ax.add_patch(ellipse)

        # Update plot limits based on detector extents
        min_x = min(min_x, center_x - radius_x)
        max_x = max(max_x, center_x + radius_x)
        min_y = min(min_y, center_y - radius_y)
        max_y = max(max_y, center_y + radius_y)

    # Plot detector centers as distinct markers
    if detector_centers_x:
      ax.scatter(detector_centers_x, detector_centers_y, marker='x', color='red', s=40,
                 label='Detector Centers', zorder=3)

# Add some padding to the limits
padding_x = (max_x - min_x) * 0.05
padding_y = (max_y - min_y) * 0.05
ax.set_xlim(min_x - padding_x, max_x + padding_x)
ax.set_ylim(min_y - padding_y, max_y + padding_y)

# Set equal scaling
ax.set_aspect('equal', adjustable='box')

ax.set_xlabel(feature_name1)
ax.set_ylabel(feature_name2)
ax.set_title('Plot 2: Original Self Data + Detector Coverage (2 Features)')
ax.legend()
ax.grid(True, linestyle='--', alpha=0.5)
plt.show()


# --- Plot 3: Combined Data (Original vs. Synthetic) ---
print("\nGenerating Plot 3: Combined Dataset (Original vs. Synthetic)...")
plt.figure(figsize=(8, 6))

# Select points based on labels
original_mask = (combined_labels == 0)
synthetic_mask = (combined_labels == 1)

# Plot original points
plt.scatter(combined_data[original_mask, 0], combined_data[original_mask, 1], # Indices 0, 1
            label='Original Self Data (Label 0)', alpha=0.7, s=30, c='blue')

# Check if there are any synthetic anomalies before plotting
if np.any(synthetic_mask):
    plt.scatter(combined_data[synthetic_mask, 0], combined_data[synthetic_mask, 1], # Indices 0, 1
                label='Synthetic Anomalies (Label 1)', alpha=0.7, s=40, c='red', marker='^')
else:
    print("Note: No synthetic anomalies were generated or included in the combined dataset.")


plt.xlabel(feature_name1)
plt.ylabel(feature_name2)
plt.title('Plot 3: Combined Dataset (Self vs. Synthetic Anomalies) (2 Features)')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.axis('equal') # Often useful for 2D anomaly detection plots
plt.show()

print("\n--- Visualizations Complete ---")

# Cell 5: Save Augmented Dataset to CSV

# --- Configuration: Set your desired output file path ---

# Option A: Save to Colab's temporary storage
# output_filename = '/content/augmented_dataset_2features_with_labels.csv'

# Option B: Save directly to your Google Drive (if mounted)
output_filename = data_base_path + 'augmented_dataset_2.csv' # <<< ADJUST if needed

# --- End Configuration ---

print("\n--- Preparing Augmented Dataset for Saving ---")

# Check if combined_data exists (should have been created in Cell 3)
if 'combined_data' not in locals():
     print("Error: 'combined_data' not found. Please run Cell 3 first.")
     raise NameError("'combined_data' is not defined")

# 1. Prepare DataFrame for Saving
# Get feature names (try from DataFrame 'df', fallback to generic names)
try:
    feature_names = df.columns[:N_FEATURES].tolist() # Should get 2 names
    if len(feature_names) != N_FEATURES: raise IndexError("Incorrect feature count from df")
    print(f"Using feature names from loaded DataFrame: {feature_names}")
except (NameError, IndexError, AttributeError):
    feature_names = [f'Feature_{i+1}' for i in range(N_FEATURES)] # Generates ['Feature_1', 'Feature_2']
    print(f"Warning: Could not get feature names from DataFrame 'df'. Using generic names: {feature_names}")


# Create a Pandas DataFrame from the combined data
df_combined = pd.DataFrame(combined_data, columns=feature_names)

# Add the labels as a new column
label_column_name = 'Is_Anomaly'
df_combined[label_column_name] = combined_labels # Add the label vector

print(f"\nFinal DataFrame preview:\n{df_combined.head()}")
print(f"\nValue counts for '{label_column_name}':\n{df_combined[label_column_name].value_counts()}")

# 2. Save to CSV
print(f"\nSaving augmented dataset to: {output_filename}")
try:
    df_combined.to_csv(output_filename, index=False)
    print("--- Save Complete ---")
    num_added = np.sum(combined_labels == 1)
    num_original = np.sum(combined_labels == 0)
    print(f"File contains {num_original} original samples (labeled 0) and {num_added} synthetic anomalies (labeled 1).")

except Exception as e:
    print(f"Error saving file: {e}")
    print("Check if the output path is valid and if you have write permissions (especially for Google Drive).")