# -*- coding: utf-8 -*-
"""rnsa hybrid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aTqqMgS5Ld1IouParK8yHJijpaZm8gGe
"""

# Cell 1: Setup and Function Definitions (RNNSA)
!pip install numpy pandas matplotlib # Ensure libraries are installed

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as patches # For detector visualization (Circles)
import random
import time
import warnings # To handle cases where fewer anomalies are generated than requested
from google.colab import drive # If using Google Drive

# --- Configuration ---
N_FEATURES = 2 # Keep as 2

# --- Helper Function: Check if a point matches a REAL-VALUED detector ---
def matches_detector_rnsa(point, detector_center, detector_radius):
    """
    Checks if a point falls within the hyper-sphere of a detector using Euclidean distance.

    Args:
        point (np.ndarray): The data point to check (1D array).
        detector_center (np.ndarray): The center of the detector (1D array).
        detector_radius (float): The radius of the detector's hyper-sphere.

    Returns:
        bool: True if the point matches the detector, False otherwise.
    """
    distance = np.linalg.norm(point - detector_center) # Euclidean distance
    return distance <= detector_radius

# --- Phase 1: Detector Generation (RNNSA) ---
def generate_detectors_rnsa(self_data, num_detectors, min_radius, max_radius, max_trials=10000):
    """Generates real-valued detectors (hyper-spheres) that do not match any self data points."""
    if self_data.ndim != 2 or self_data.shape[1] != N_FEATURES:
        raise ValueError(f"self_data must be a 2D array with {N_FEATURES} features (columns). Shape received: {self_data.shape}")

    mature_detectors = []
    n_self_samples = self_data.shape[0]
    data_min = np.min(self_data, axis=0)
    data_max = np.max(self_data, axis=0)

    # Ensure radii are single float values
    if not isinstance(min_radius, (int, float)) or not isinstance(max_radius, (int, float)):
        raise ValueError("For RNNSA, min_radius and max_radius must be single float values.")
    if min_radius > max_radius:
        raise ValueError("min_radius cannot be greater than max_radius.")
    if min_radius < 0:
        raise ValueError("min_radius cannot be negative.")

    print(f"Starting RNNSA detector generation for {num_detectors} detectors ({N_FEATURES} features)...")
    start_time = time.time()
    trials = 0

    while len(mature_detectors) < num_detectors and trials < max_trials:
        trials += 1

        # 1. Generate candidate detector center within data bounds (or slightly expanded)
        range_expansion = (data_max - data_min) * 0.1
        gen_min = data_min - range_expansion
        gen_max = data_max + range_expansion
        candidate_center = np.random.uniform(gen_min, gen_max, size=N_FEATURES)

        # 2. Generate candidate SINGLE radius within specified range
        candidate_radius = np.random.uniform(min_radius, max_radius)

        # 3. Check if the candidate detector matches any self point using RNNSA rule
        is_valid = True
        for i in range(n_self_samples):
            self_point = self_data[i, :]
            if matches_detector_rnsa(self_point, candidate_center, candidate_radius):
                is_valid = False
                break

        # 4. If it doesn't match any self point, add it to mature detectors
        if is_valid:
            mature_detectors.append({
                'center': candidate_center,
                'radius': candidate_radius # Store the single radius
            })
            if len(mature_detectors) % (num_detectors // 10 + 1) == 0:
                 print(f"  Generated {len(mature_detectors)} / {num_detectors} detectors... (trial {trials})")

    end_time = time.time()
    print(f"Detector generation finished in {end_time - start_time:.2f} seconds.")
    print(f"Generated {len(mature_detectors)} detectors after {trials} trials.")

    if trials >= max_trials and len(mature_detectors) < num_detectors:
        warnings.warn(f"Warning: Reached max_trials ({max_trials}) but only generated "
                      f"{len(mature_detectors)} out of {num_detectors} desired detectors. "
                      "Consider increasing max_trials, adjusting radius range, or checking self-data distribution.")

    return mature_detectors

# --- Phase 2: Monitoring / Classification (RNNSA) ---
def classify_point_rnsa(point, mature_detectors):
    """Classifies a data point using RNNSA detectors."""
    if point.shape[0] != N_FEATURES:
         raise ValueError(f"Point must have {N_FEATURES} features.")

    for detector in mature_detectors:
        # Use the RNNSA matching function
        if matches_detector_rnsa(point, detector['center'], detector['radius']):
            return 'anomaly'

    return 'normal'

# --- Phase 3: Synthetic Anomaly Generation (RNNSA) ---
def generate_synthetic_anomalies_rnsa(mature_detectors, num_synthetic_anomalies):
    """Generates synthetic anomaly data points based on RNNSA detectors (hyper-spheres)."""
    if not mature_detectors or num_synthetic_anomalies <= 0:
        return np.empty((0, N_FEATURES))

    num_available_detectors = len(mature_detectors)
    num_to_generate = min(num_synthetic_anomalies, num_available_detectors)

    if num_synthetic_anomalies > num_available_detectors:
        warnings.warn(f"Requested {num_synthetic_anomalies} synthetic anomalies, but only "
                      f"{num_available_detectors} detectors are available. Generating {num_to_generate} anomalies.")

    synthetic_points = []
    chosen_indices = np.random.choice(num_available_detectors, size=num_to_generate, replace=True)

    print(f"\nGenerating {num_to_generate} synthetic anomalies using RNNSA detectors...")
    for i in chosen_indices:
        detector = mature_detectors[i]
        center = detector['center']
        radius = detector['radius']

        # Generate a point uniformly within the 2D circle (hyper-sphere)
        # Use square root of uniform sample for radius distribution to avoid center bias
        r_sample = np.sqrt(np.random.uniform(0, 1)) * radius
        theta = np.random.uniform(0, 2 * np.pi)

        # Calculate offset from center
        offset = np.array([r_sample * np.cos(theta), r_sample * np.sin(theta)])

        synthetic_point = center + offset
        synthetic_points.append(synthetic_point)

    print("Synthetic anomaly generation complete.")
    return np.array(synthetic_points)

# Cell 2: Mount Drive (Optional) and Load Data

# --- Option A: Mount Google Drive (Recommended for persistence) ---
# drive.mount('/content/drive')
# data_base_path = '/content/drive/MyDrive/Colab_Data/' # <<< ADJUST PATH in your Drive

# --- Option B: Use Colab Temporary Storage (Upload file manually) ---
data_base_path = '/content/' # Base path for direct uploads

# --- Specify your DATASET filename (must have 2 features) ---
data_filename = data_base_path + 'selected_features_dataset_5.csv' # <<< CHANGE FILENAME

print(f"Attempting to load self data from: {data_filename}")
try:
    # Using Pandas (Recommended - handles headers automatically)
    df = pd.read_csv(data_filename, delimiter=',') # Adjust delimiter if needed
    df = df.drop(df.columns[-1], axis=1)
    df = (df - df.min()) / (df.max() - df.min())

    # Check number of columns BEFORE converting to NumPy
    if df.shape[1] != N_FEATURES:
         raise ValueError(f"Loaded DataFrame has {df.shape[1]} columns, but expected {N_FEATURES}.")

    self_data = df.values # Convert the Pandas DataFrame to a NumPy array AFTER check

    print(f"Successfully loaded {self_data.shape[0]} samples with {self_data.shape[1]} features.")

except FileNotFoundError:
    print(f"Error: Data file '{data_filename}' not found.")
    print("Check the path: Did you upload the file? Did you mount Drive correctly? Is the filename correct?")
    raise # Stop execution
except ValueError as ve: # Catch the specific column count error
    print(f"Data Loading Error: {ve}")
    raise # Stop execution
except Exception as e:
    print(f"Error loading data file: {e}")
    print("Check file format, delimiter, and ensure it contains only numeric data.")
    raise # Stop execution

# Display basic info about the loaded data
print(f"\nOriginal Self Data Shape: {self_data.shape}")
print(f"Self Data Min per feature: {np.min(self_data, axis=0)}")
print(f"Self Data Max per feature: {np.max(self_data, axis=0)}")
print("-" * 30)

# Cell 3: Generate Detectors and Synthetic Anomalies (RNNSA)

# --- Hyperparameters ---
NUM_TARGET_DETECTORS = 466  # Adjust as needed
# Radii are now SINGLE floats for RNNSA
MIN_RADIUS = 0.2             # <<< EXAMPLE: Single float value
MAX_RADIUS = 1.5             # <<< EXAMPLE: Single float value
MAX_GENERATION_TRIALS = 40000 # Adjust as needed
NUM_SYNTHETIC_ANOMALIES = 466 # Adjust as needed
# --- End Hyperparameters ---

# 1. Generate Detectors using RNNSA function
detectors = generate_detectors_rnsa( # <<<< Calling RNNSA function
    self_data,
    NUM_TARGET_DETECTORS,
    MIN_RADIUS,              # <<<< Pass single min radius
    MAX_RADIUS,              # <<<< Pass single max radius
    MAX_GENERATION_TRIALS
)
print("-" * 30)

# 2. Generate Synthetic Anomalies using RNNSA function
if not detectors:
    print("No detectors were generated. Cannot generate synthetic data.")
    synthetic_anomalies = np.empty((0, N_FEATURES)) # Create empty array
else:
    synthetic_anomalies = generate_synthetic_anomalies_rnsa( # <<<< Calling RNNSA function
        detectors,
        NUM_SYNTHETIC_ANOMALIES
    )

if synthetic_anomalies.shape[0] > 0:
    print(f"Generated Synthetic Anomaly Data Shape: {synthetic_anomalies.shape}")
else:
    print("\nNo synthetic anomalies were generated (either 0 requested or no detectors available).")

# 3. Combine Original Self Data and Synthetic Anomalies
# (No change in logic here)
combined_data = np.vstack((self_data, synthetic_anomalies))

# Create corresponding labels (0 for self, 1 for synthetic anomaly)
self_labels = np.zeros(self_data.shape[0], dtype=int)
anomaly_labels = np.ones(synthetic_anomalies.shape[0], dtype=int)
combined_labels = np.concatenate((self_labels, anomaly_labels))

print(f"\nCombined Dataset Shape: {combined_data.shape}")
print(f"Combined Labels Shape: {combined_labels.shape}")
print(f"Number of original self samples in combined set: {np.sum(combined_labels == 0)}")
print(f"Number of synthetic anomalies in combined set: {np.sum(combined_labels == 1)}")
print("-" * 30)

# Cell 4: 2D Visualizations (RNNSA)

# Extract feature names (same as before)
try:
    feature_name1 = df.columns[0]
    feature_name2 = df.columns[1]
except (NameError, IndexError, AttributeError):
    feature_name1 = 'Feature 1'
    feature_name2 = 'Feature 2'

print(f"\n--- Generating 2D Visualizations (RNNSA) using '{feature_name1}' and '{feature_name2}' ---")

# --- Plot 1: Original Self Data ---
# (No change from previous 2-feature version)
print("Generating Plot 1: Original Self Data...")
plt.figure(figsize=(8, 6))
plt.scatter(self_data[:, 0], self_data[:, 1], label='Original Self Data', alpha=0.7, s=30, c='blue')
plt.xlabel(feature_name1)
plt.ylabel(feature_name2)
plt.title('Plot 1: Original Self Data Points (2 Features)')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.axis('equal')
plt.show()


# --- Plot 2: Original Data + Detectors (Centers and Radii - CIRCLES) ---
print("\nGenerating Plot 2: Original Self Data + RNNSA Detectors...")
fig, ax = plt.subplots(figsize=(10, 8))

# Plot self data first
ax.scatter(self_data[:, 0], self_data[:, 1], label='Original Self Data', alpha=0.6, s=30, c='blue', zorder=1)

# Determine plot limits based on data and detector extents
min_x, max_x = np.min(self_data[:, 0]), np.max(self_data[:, 0])
min_y, max_y = np.min(self_data[:, 1]), np.max(self_data[:, 1])

detector_centers_x = []
detector_centers_y = []

if detectors:
    for detector in detectors:
        center = detector['center']
        radius = detector['radius'] # <<<< Get single radius

        center_x, center_y = center[0], center[1]
        detector_centers_x.append(center_x)
        detector_centers_y.append(center_y)

        # Create a CIRCLE patch for RNNSA
        circle = patches.Circle((center_x, center_y), radius=radius, # <<<< Use radius here
                                edgecolor='red', facecolor='none', linestyle='-', linewidth=1,
                                alpha=0.5, label='_nolegend_', zorder=2)
        ax.add_patch(circle)

        # Update plot limits based on detector extents
        min_x = min(min_x, center_x - radius) # Use single radius
        max_x = max(max_x, center_x + radius)
        min_y = min(min_y, center_y - radius)
        max_y = max(max_y, center_y + radius)

    # Plot detector centers
    if detector_centers_x:
      ax.scatter(detector_centers_x, detector_centers_y, marker='x', color='red', s=40,
                 label='Detector Centers', zorder=3)

# Add padding and set limits
padding_x = (max_x - min_x) * 0.05
padding_y = (max_y - min_y) * 0.05
ax.set_xlim(min_x - padding_x, max_x + padding_x)
ax.set_ylim(min_y - padding_y, max_y + padding_y)

# Set equal scaling for circles
ax.set_aspect('equal', adjustable='box')

ax.set_xlabel(feature_name1)
ax.set_ylabel(feature_name2)
ax.set_title('Plot 2: Original Self Data + RNNSA Detector Coverage (Circles)')
ax.legend()
ax.grid(True, linestyle='--', alpha=0.5)
plt.show()


# --- Plot 3: Combined Data (Original vs. Synthetic) ---
# (No change from previous 2-feature version)
print("\nGenerating Plot 3: Combined Dataset (Original vs. Synthetic)...")
plt.figure(figsize=(8, 6))
original_mask = (combined_labels == 0)
synthetic_mask = (combined_labels == 1)
plt.scatter(combined_data[original_mask, 0], combined_data[original_mask, 1], label='Original Self Data (Label 0)', alpha=0.7, s=30, c='blue')
if np.any(synthetic_mask):
    plt.scatter(combined_data[synthetic_mask, 0], combined_data[synthetic_mask, 1], label='Synthetic Anomalies (Label 1)', alpha=0.7, s=40, c='red', marker='^')
else:
    print("Note: No synthetic anomalies were generated or included.")
plt.xlabel(feature_name1)
plt.ylabel(feature_name2)
plt.title('Plot 3: Combined Dataset (Self vs. Synthetic Anomalies) (RNNSA)')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.axis('equal')
plt.show()

print("\n--- Visualizations Complete ---")

# Cell 5: Save Augmented Dataset to CSV

# --- Configuration: Set your desired output file path ---
# Option A: Save to Colab's temporary storage
# output_filename = '/content/augmented_dataset_RNNSA_2features_with_labels.csv'
# Option B: Save directly to your Google Drive (if mounted)
output_filename = data_base_path + 'augmented_dataset_RNSA_2.csv' # <<< ADJUST if needed

# --- End Configuration ---

print("\n--- Preparing Augmented Dataset for Saving (RNNSA) ---")

if 'combined_data' not in locals():
     print("Error: 'combined_data' not found. Please run Cell 3 first.")
     raise NameError("'combined_data' is not defined")

# 1. Prepare DataFrame
try:
    feature_names = df.columns[:N_FEATURES].tolist()
    if len(feature_names) != N_FEATURES: raise IndexError("Incorrect feature count from df")
    print(f"Using feature names from loaded DataFrame: {feature_names}")
except (NameError, IndexError, AttributeError):
    feature_names = [f'Feature_{i+1}' for i in range(N_FEATURES)]
    print(f"Warning: Could not get feature names from DataFrame 'df'. Using generic names: {feature_names}")

df_combined = pd.DataFrame(combined_data, columns=feature_names)
label_column_name = 'Is_Anomaly'
df_combined[label_column_name] = combined_labels

print(f"\nFinal DataFrame preview:\n{df_combined.head()}")
print(f"\nValue counts for '{label_column_name}':\n{df_combined[label_column_name].value_counts()}")

# 2. Save to CSV
print(f"\nSaving augmented dataset to: {output_filename}")
try:
    df_combined.to_csv(output_filename, index=False)
    print("--- Save Complete ---")
    num_added = np.sum(combined_labels == 1)
    num_original = np.sum(combined_labels == 0)
    print(f"File contains {num_original} original samples (labeled 0) and {num_added} synthetic anomalies (labeled 1).")

except Exception as e:
    print(f"Error saving file: {e}")
    print("Check if the output path is valid and if you have write permissions.")